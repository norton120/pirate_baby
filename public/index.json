[{"content":"\u0026#x1f911; Asking Price: $23,000 Description (from YachtWorld) 1978 Sabre Mk II 34'\nThis Sabre has been very well kept and maintained by its two only owners. The Sabre 34 was one of Sabre\u0026rsquo;s most successful designs with a long production life. This boat features a functional interior with beautiful simple lines that keep the \u0026lsquo;34\u0026rsquo; popular with sailors who want performance as well as comfort. The interiors are crafted in teak wood with high quality joinery work throughout.\nThis Sabre 34 is a USCG documented vessel.Â The perfect weekend cruiser, overnighter, or racer as Sabre sailboats are known to be fast and point well. Coral Reefer II can comfortably sleep six and she is looking for an owner that loves solid sailboats to take her for the new leg of her journey.\nUPGRADES \u0026amp; MAINTENANCEÂ ENGINE FULLY SERVICED (2022)Â NEW MOTOR MOUNTS \u0026amp; INJECTORS (2022)\nNEW HEAT PUMP (2021)\nREPLACED AC UNIT (2021)\nGPS, WIND, SPEED, DEPTH INSTALLATION (2021)\nBOTTOM PAINT (2021)\nNEW FRESH WATER PUMP \u0026amp; FAUCET IN GALLEY (2021)\nNEW HEAD (2021)\nREPLACED GAS STOVE WITH MICROWAVE, HAS STOVE WITH CNG TANKS, ETC.\nTWO EXTRA SAILSÂ WINTER COVER, LOTS OF PARTS, PAINT, SPARES, FANS, AND MORE\nBRAND NEW 3.5HP MERCURY ENGINE WITH 9\u0026rsquo; TOBIN SPORT DINGY, BOTH NEVER USED NOT INCLUDEDÂ Please contact Regent Point Yacht Sales Brokerage Team atÂ (804) 758-4457\nðŸ“· Images: Basic Specifications beam: 10.42 heads: 1 fuel_tank: Unknown max_draft: 5.5 holding_tank: Unknown length_overall: 35.5 fresh_water_tank: Unknown Engine Details fuel_type: Diesel engine_make: Volvo Penta engine_type: Inboard engine_year: 1978 engine_model: m13 engine: Volvo Penta m13 engine hours: Unknown total power: Unknown ","permalink":"http://0.0.0.0:1313/boats/1978-sabre-34-8762563/","summary":"\u0026#x1f911; Asking Price: $23,000 Description (from YachtWorld) 1978 Sabre Mk II 34'\nThis Sabre has been very well kept and maintained by its two only owners. The Sabre 34 was one of Sabre\u0026rsquo;s most successful designs with a long production life. This boat features a functional interior with beautiful simple lines that keep the \u0026lsquo;34\u0026rsquo; popular with sailors who want performance as well as comfort. The interiors are crafted in teak wood with high quality joinery work throughout.","title":"1978 Sabre"},{"content":"\u0026#x1f911; Asking Price: $17,000 Description (from YachtWorld) MAINTENANCE \u0026amp; UPGRADESÂ ENGINE FULLY SERVICED (FUEL FILTER, GLOW PLUGS \u0026amp; NEW LIFT PUMP)\n(JANUARY 2023)Â CUSTOM LUX FOAM MATTRESS IN V- BERTH (FEBRUARY 2023)\nNEW MAINSAIL COVER \u0026amp; COCKPIT CANVAS (FEBRUARY 2022)Â NEW INTERIOR CUSHIONS (APRIL 2022)Â Blue HeronÂ has stood the test of time, remaining highly sought after on the used boat market even after almostÂ 25 yearsÂ since its introduction. Her previous owners, who were the second owners, sailed her to Bermuda twice. The current sellers have taken good care of her and are now ready to sell. With a traditional design and comfortable living space both on deck and below, the Pearson 35 is a great option for sailors looking for a classic vessel with lasting qualities.\nðŸ“· Images: Basic Specifications beam: 10.0 heads: 1 fuel_tank: Unknown max_draft: 7.5 holding_tank: Unknown cabin_headroom: 6.25 length_overall: 35.0 fresh_water_tank: Unknown Engine Details fuel_type: Diesel engine_make: Westerbeke engine_model: W30 engine: Westerbeke W30 engine hours: Unknown total power: Unknown ","permalink":"http://0.0.0.0:1313/boats/1981-pearson-35-pearson-9126528/","summary":"\u0026#x1f911; Asking Price: $17,000 Description (from YachtWorld) MAINTENANCE \u0026amp; UPGRADESÂ ENGINE FULLY SERVICED (FUEL FILTER, GLOW PLUGS \u0026amp; NEW LIFT PUMP)\n(JANUARY 2023)Â CUSTOM LUX FOAM MATTRESS IN V- BERTH (FEBRUARY 2023)\nNEW MAINSAIL COVER \u0026amp; COCKPIT CANVAS (FEBRUARY 2022)Â NEW INTERIOR CUSHIONS (APRIL 2022)Â Blue HeronÂ has stood the test of time, remaining highly sought after on the used boat market even after almostÂ 25 yearsÂ since its introduction. Her previous owners, who were the second owners, sailed her to Bermuda twice.","title":"1981 Pearson"},{"content":"preamble The drive home from the movie theater was not going well. My then-girlfriend sat arms crossed in the passenger seat, the red glow from the traffic light matching her mood. \u0026ldquo;You can\u0026rsquo;t just let it be romantic. Why do you have to ruin everything!?!\u0026rdquo; We had just seen 50 First Dates, a classic Sandler/Barrymore romance about a woman who\u0026rsquo;s brain injury prevents her from forming long-term memories. In the movie, Lucy (Barrymore) constructs her own \u0026ldquo;external memory\u0026rdquo; via her diary; a clever plot device, but one that required suspension of disbelief that I just could not abide. I had done shorthand math while the credits were rolling: If Lucy\u0026rsquo;s average diary entry took 20 minutes to write (that is less than most people, and most people aren\u0026rsquo;t trying to compensate for brain damage), each entry would take roughly half that time - so about 10 minutes - to read. Reading a week\u0026rsquo;s entries would take more than an hour. By the 6 month mark, the daily catch-up on her past would require more hours of reading than are in a day. Romantic? Yes. Realistic? No. And so we argued. I believe the conversation went something like this:\n\u0026quot;Her: But what if every Sunday she made wrote a cheat sheet for the past week, and then she only read those? That would take less time. \u0026ldquo; \u0026rdquo;Me: Even a weekly summary would become unreadable in less than a year.\u0026quot; \u0026quot;Her: OK, then what if she summarized those cheat sheets?? She could keep making the history smaller and smaller.\u0026quot; \u0026quot;Me: Yeah but eventually she\u0026rsquo;d lose too much detail and the summaries would be useless.\u0026quot; \u0026quot;Her: But she\u0026rsquo;d still have her daily journals for when she needs those details!\u0026quot; \u0026quot;Me: How would she ever search that? We\u0026rsquo;re back where we started.\u0026quot;\nTwenty years later, the \u0026ldquo;Lucy problem\u0026rdquo; is a perfect lens to help us understand one of the most important challenges in designing a Large Language Model Agent Framework. The solution proposed by researchers at UC Berkeley is remarkably innovative and offers exciting potential - and it is a solution that bears significant resemblance to the \u0026ldquo;Lucy solution\u0026rdquo; I was arguing against during that car ride home. It looks like I owe someone a long overdue apology.\nLucy the language model: a near-perfect analogy Large Language Models are, in reality, just functions. You input at least one argument (text) and they output in kind. This output is the product of the model\u0026rsquo;s business logic, combined parameters, and internal arguments - one of those arguments being the training data used to develop the inference model. This training data serves as the model\u0026rsquo;s \u0026ldquo;memories\u0026rdquo;; without it the LLM would output very little of value, similar to attempting a deep conversation with a newborn. The training data \u0026ldquo;memories\u0026rdquo; in an large language model are fixed at inference time, exactly like Lucy\u0026rsquo;s memories in the movie. She has developed experiences and gathered information up to a very specific point (in her case, the day of her accident); from that day forward, she interprets stimuli based on the exact state of her mind, her memories, at that time. This is precisely how inference with a large language model operates - fixed to the moment the training was complete, and the resulting function was pickled.\nEach time the LLM function is executed (here we will refer to this combined execution and response as a turn, borrowing from chat nomenclature) is exactly like one single day in the life of Lucy. With the model temperature turned down to 0 (deterministic) each turn with the same input will look exactly like Lucy\u0026rsquo;s early routine - repeating the same day over and over (and baking a lot of identical birthday cakes). An LLM cannot form new \u0026ldquo;memories\u0026rdquo; as a pure inference endpoint, any more than Lucy can.\nTo compensate for this with an LLM, the natural next step is to prepend those new \u0026ldquo;memories\u0026rdquo; as part of the text passed to the LLM function effectively augmenting the training data of the language model for the duration of the turn1. However language model context windows - the combined amount of text that can be input and output in a single turn - are limited in size. Again, this is exactly how Barrymore\u0026rsquo;s character experiences the world; her context window is one single day. Just as I argued so many years earlier that Lucy\u0026rsquo;s memories would eventually take longer to consume than there are hours in a day for her to retain them, new knowledge that must be included in a turn in order for the language model to produce a useful output quickly outgrows the available context window.\nThe limits of prompt engineering The lion\u0026rsquo;s share of LLM Engineering coverage has been devoted to prompt engineering, or crafting the content we submit in a turn so that it produces the most desirable outcome. An entire ecosystem has rapidly developed around prompt design, from prompt engineering classes to prompt exchange marketplaces - all from the idea that from the \u0026ldquo;perfect prompt\u0026rdquo; you can coax the \u0026ldquo;perfect output.\u0026rdquo;\nHenry, Sandler\u0026rsquo;s character in 50 First Dates, may have been one of the earliest prompt engineers. Early in the film Henry falls in love with Lucy and agrees not to tell her about her injury, instead wooing her anew each day. His daily \u0026ldquo;prompts\u0026rdquo; to re-win her heart begin abysmally, with most ending in rejection. Over time his technique evolves until Lucy consistently falls for him every day. We see this same example in countless language model demos, where a meticulously crafted prompt is used to visualize analytics for a dataset, or generate a spot-on cover letter.\nThe examples are impressive, but how useful is this prompting really? In the movie, Henry finally addresses the extreme limitations in a life of infinite first dates, and tells Lucy about her condition. With a language model, a \u0026ldquo;perfect prompt\u0026rdquo; executed in isolation is just as limited in value. Complex tasks require many complex steps, each building on a modified state - and this cannot be accomplished in a single turn. While prompt engineering is certainly an important piece of the puzzle, it isn\u0026rsquo;t remotely a holistic solution to our problem.\nRAG, a newspaper, and a video tape For both Lucy and the language model, things get interesting once we start externalizing memories. Retrieval Augmented Generation (RAG) is probably a close second to prompt engineering in the sheer volume of attention paid in LLM-related content. RAG can be more simply stated as \u0026ldquo;store text somewhere, then on each turn search that text and add bits to the prompt.\u0026rdquo; The most common RAG implementations today are blind semantic searches, where every user input is searched against the RAG store by semantic similarity, and then the top few search results are combined with the user input as the prompt. They look something like this:\n# prompt with just user input Question: What is the last thing Todd said before he quit yesterday? vs\n# prompt with vector similarity search results for \u0026#34;What is the last thing Todd said before he quit yesterday?\u0026#34; via embeddings, prepended to prompt Context: \u0026#34;Margo: Todd is quitting today!\u0026#34; \u0026#34;Todd: I am quitting today. I\u0026#39;ve had enough.\u0026#34; \u0026#34;Clark: I can\u0026#39;t believe Todd finally quit, Margo is going to freak.\u0026#34; Question: What is the last thing Todd said before he quit yesterday? The context injected by RAG might be very helpful, or it might be virtually irrelevant. What\u0026rsquo;s more, the question may not require the context at all, and the RAG may just be noise.\nAgain 50 First Dates does not disappoint with real-world analogs. In the film, Lucy\u0026rsquo;s condition is kept hidden from her with the help of falsified context clues; her father swaps out her newspaper with a reprinted one, passes off a recorded football game as live TV, and paints over a wall every evening so she can re-paint it the next day, none the wiser. This context adds to the prompt and allows Lucy to live a full day (albeit the same one over and over). It does a significantly better job of reaching the desired outcome (Lucy enjoys her day and is able to function within it) than relying completely on the day\u0026rsquo;s organic events. Later, Henry introduces the first attempt to be honest with Lucy in the form of a VHS recording. To the plot of the film this is a pivotal moment, as it is Lucy\u0026rsquo;s first step towards regaining agency. With the language model, it is functionally the same as the newspaper and the paint; each turn is potentially better and more informed when it includes RAG content, but it is still very much an isolated turn without true external state management.\nRegardless of which Lucy consumes - the fake newspaper or Henry\u0026rsquo;s real VHS tape - improvement in Lucy\u0026rsquo;s life is limited to the outcome of that day. Lucy still has no agency to live a full life, just as our language model can take no meaningful steps towards completing a complex task.\nJust like prompt engineering, RAG is a piece of the puzzle, but it is not the answer in and of itself.\nA mind with a diary Let\u0026rsquo;s review that theory from the car. What if Lucy kept a diary, and then managed this \u0026ldquo;external memory\u0026rdquo; by summarizing, consolidating, and making herself cheat sheets? Unlike her father\u0026rsquo;s newspapers or Henry\u0026rsquo;s VHS tapes, this memory would be completely under her control. She decides what information is critically important, what memories can be forgotten, and what knowledge should live on in \u0026ldquo;cold storage\u0026rdquo; to be dredged up only when required. The film touches on this idea (though it took my romance-ruining logic to really see it played out in detail). With an external memory like this, Lucy is now unbound from her context window. She can pursue her interests for as long as they take, participate actively in the mundane but important events of life, have a family, live. She can make a menu for the week on Monday, go shopping for groceries Wednesday, and cook them on Friday - all the elements of agency returned to her by a few notebooks and todo lists.\nThis is remarkably similar to the premise behind the MemGPT project2. The context window, chat history, and RAG knowledge store are treated as tiered \u0026ldquo;memories\u0026rdquo;; each memory can be written and purged by the LLM using tool calls. Instead of approaching each turn as an isolated prompt + RAG + input = response, the \u0026ldquo;core\u0026rdquo; memory (the context window content) is processed continually as a daemon, with a new turn on each \u0026ldquo;heartbeart.\u0026rdquo; Just as each day Lucy is presented with her notes and todo list from the previous day, with each turn the language model is presented with the core memory assembled by the previous turn. During that turn the language model can edit this core memory, move information out of core and into archival memory, or search/recall from archival memory and add that back to core, and these changes will apply to the core memory presented in the next turn.\nHere\u0026rsquo;s a walk-through of how an agent could work (with a bit of pseudo-code) to answer a user\u0026rsquo;s question about a company. The agent has a core memory describing itself and the user, a history section containing the last five messages, and a list of tools the agent can call. This core memory is passed as the prompt in a turn every 10 seconds, or sooner if externally triggered.\nOn the first pass the agent calls the archive_lines() tool to move the 2nd paragraph of the user information to cold storage. Why? because the system prompt informs the agent of the current and max memory sizes, and instructs the agent to self-adjust when needed. On the next pass, the tool call request and result are the newest entries in the core memory history (the 2 oldest from the last turn have been moved to recall memory and are not visible). This turn the agent calls pause_heartbeats() to wait for external stimuli, as it detects nothing else to update in it\u0026rsquo;s own core memory. The next turn is triggered by a user message, which is the newest history entry in this turn\u0026rsquo;s core memory. The user message says \u0026ldquo;how many people are in the Seattle office?\u0026rdquo; and the agent\u0026rsquo;s first action is to call the get_employee_list() tool, along with the internal monologue message \u0026ldquo;I must get the list of employees and count only the ones in the Seattle office. I must then send the message back to the user\u0026rdquo; . On the next turn, the agent tool call, internal monologue, and tool call result are in the core memory history, but the original user message has been pushed out. On this turn the agent\u0026rsquo;s internal monologue is \u0026ldquo;65 employees are in the Seattle office. I must retrieve the original question so I can answer it correctly\u0026rdquo; and the tool call is retrieve_message(user, -1) to get the last message from the user. On the next turn, the agent calls send_message(\u0026quot;there are 65 employees in the Seattle office\u0026quot;), and responds to the user. And the beat goes on.\nThis \u0026ldquo;language model operating system\u0026rdquo; is a starkly contrasting view of how inference services could function to complete complex, multifaceted tasks. It also changes the paradigm of \u0026ldquo;evolution\u0026rdquo; as an agent, allowing the application to adjust and self-correct. Prompts and RAG become correctable elements within a generative \u0026ldquo;cycle\u0026rdquo;; if a RAG search is ineffective or prompt misses the mark, it can be re-tried or compensated for on the next turn. Most distinctly important from single-turn agent design, the results of this self-managed memory are cumulative. This is an absolute necessity for true agency.\nI am very excited about what a framework built on this concept could mean; adding stimuli to a well-appointed agent (or cluster of agents) becomes an execution layer that evolves beyond text generation and an ROI that grows exponentially with the complexity of its\u0026rsquo; charge. A language model operating in this fashion is still a language model - a function, not a sentient being - but it crosses a threshold of appearance that is the stuff of Sci-Fi. More importantly, it adds a critical element to the generative equation that I just don\u0026rsquo;t see autonomous agency succeeding without: repetition. Humans don\u0026rsquo;t immediately think every required thought and blurt out the perfect response in one breath; we take steps, ask questions that uncover new questions, pause to consider internally, and arrive at an answer. By bestowing that same capacity on an application, this language model operating system could be a new paradigm in computing.\nFor those of us building user-facing agents, this is a concept worth focus and cycles. Single-turn prompt libraries and slice-and-dice embedding building to RAG away bad responses was the best idea we had in the dark AI days of 6 months ago, but I don\u0026rsquo;t think they will get us where we want to go. In 50 First Dates Lucy married Henry, became a mom, and sailed to Alaska, all because she was restored the agency to own her mind and manage her memories. Language model agents need the same in an \u0026ldquo;operating system\u0026rdquo; if we want to unlock them.\nJoin me next time, when we explore the parallels between Happy Gilmore and K8S (hint: there are none).\nFootnotes: 1. Adding context to a prompt and fine-tuning or retraining a model are not really the same thing, but I was willing to take a few liberties with technical accuracy for the sake of clearly demonstrating the subject concepts.\n2. Note that throughout this writing I am referring to the concepts introduced by the research behind MemGPT, not the implementation itself. The nomenclature, pseudo-code, and description of events here are not intended to reflect the software project.\nMemGPT Citation: packer 2023 memgpt, MemGPT: Towards LLMs as Operating Systems, authors: Packer, Charles and Wooders, Sarah and Lin, Kevin and Fang, Vivian and Patil, Shishir G. and Stoica, Ion and Gonzalez, Joseph E., arXiv preprint arXiv:2310.08560\n","permalink":"http://0.0.0.0:1313/posts/50_first_dates_with_memgpt/","summary":"preamble The drive home from the movie theater was not going well. My then-girlfriend sat arms crossed in the passenger seat, the red glow from the traffic light matching her mood. \u0026ldquo;You can\u0026rsquo;t just let it be romantic. Why do you have to ruin everything!?!\u0026rdquo; We had just seen 50 First Dates, a classic Sandler/Barrymore romance about a woman who\u0026rsquo;s brain injury prevents her from forming long-term memories. In the movie, Lucy (Barrymore) constructs her own \u0026ldquo;external memory\u0026rdquo; via her diary; a clever plot device, but one that required suspension of disbelief that I just could not abide.","title":"50 First Dates With MemGPT"},{"content":"History The most money I have ever made was my years spent selling motorcycles. Software Engineering is by all means a lucrative profession, and I\u0026rsquo;ve never been the starving artist developer who prizes a compelling mission over a compelling paycheck. But the years I sold motorcycles - between 2003 and 2008 - coincided with a wave of temporary insanity in the US real estate market. Heretofore normal people became mad with the sudden access to \u0026ldquo;wealth,\u0026rdquo; in the form of massive home equity reserves they could spend as easily as cash via HELOC checkbooks. And spend they did - in my dealership, to the tune of $100,000 boats, $50,000 Harley Davidsons, and $30,000 jetskis, often two or three at a time (\u0026ldquo;get the pair or the garage will look lopsided\u0026rdquo;). This was a time when ex-convict, high-school dropout motorcycle salesman would net twice the income of the Ivy-league lawyer to whom they were selling an overpriced jetboat. As salespeople we assumed this prosperity to be an eternal constant, and we planned, spent, and generally behaved accordingly. The day after Lehman Brothers toppled in the fall of 2008, I checked that the front doors of the dealership were unlocked. Twice. Before that day, I could look out the window of my elevated office and see thousands of customers spread across our dealership floor, a football field of motorcycle commerce in action. From that day forward, the customers numbered in single digits and the floor was populated only by listless, terrified salespeople. Salespeople that saw their incomes vanish and their savings evaporate; most went from six-figure incomes to earning little more than minimum wage overnight. But a short time into this barren economic winter, there was a sudden reprieve in the form of the Southern California gas price explosion. Economic forces combined to send the price of a gallon to just under $5 (that\u0026rsquo;s over $7 a gallon today), and Angelinos again panicked - this time stampeding to our dealership to purchase 70-plus-mpg scooters and motorcycles. At one point we partnered with a local car dealer so customers could trade in their massive SUVs and lifted trucks (for a sizable loss) and pay top dollar for a Vespa clone. By all accounts the gravy days were back, and most of the salespeople resumed decadence as usual. But the seasoned ones, the wise old oaks (probably in their 30\u0026rsquo;s now that I think of it) maintained caution. They used this temporary reprieve to right-size, to reposition themselves. One sold his mansion (there is no other word for that house) to an eager young co-worker for a sizable, but still tolerable, loss. Many left management roles and returned to the showroom floor, preparing for the impending downsizing (and reduced need for management overhead). Of course the scooter bubble quickly burst, and what was left was nearly half a decade of the worst sales and slowest rebound in the history of power sports. It was only the seasoned veterans that survived.\nHistory Repeats Itself I feel I will tell a very similar story of Software/Data/DevOps (et al) Engineering in the not-to-distant future. A slow-burning period of unsustainable gluttony followed immediately by a devastating correction, then a moment of opportunity, mistaken by most to be a return of \u0026ldquo;the good times\u0026rdquo;, and finally, a long plunge into sensible reality. The first two parts, past decadence and inevitable correction, are common narratives at the moment so I\u0026rsquo;ll only touch on them enough to set the stage. What I think matters today, and the heart of this writing, is a dissection of that critical third part - the window of opportunity - and the sanity that is to follow.\nIt was the best of times We all know the stories of zero-rate excess. (M/F)AANG employees outraged that their favorite cereal is no longer stocked at the closest snack station, CompSci grads dumbfounded that their first job would expect a five-day work week. Then there are the more mundane (but no less tell-tale) evidences: recruiters hounding anyone titled \u0026ldquo;Developer\u0026rdquo; with sight-unseen six-figure job offers, lavish office perks that sounded more like a cruise ship than a place of business (\u0026ldquo;we have a climbing wall!\u0026rdquo;). Rock-star impunity as \u0026ldquo;the talent,\u0026rdquo; in great demand and impossible to replace. And of course, a shared cognitive dissidence ignoring the lack of concrete ROI from all of it.\nIt was the worst of times We all know these stories too. (M/F)AANG tech layoffs in the range of 40,000 people with more on the way. 320,000 across all of tech in the last year. VC funding at the lowest point in six years, 62% lower than the funding peak in \u0026lsquo;22. Layers of software management stripped away, initiatives shit-canned, and post after painful social media post from displaced big tech Engineers who can\u0026rsquo;t seem to find their next role. All the while, a looming fear of Artificial Intelligence that might quickly automate away roles that we once believed to be sacredly human.\nThe Indian summer During these dark times OpenAI grew from $29B to over $80B in value. Twenty new AI unicorns were minted in \u0026lsquo;23 (the slowest year for new unicorns in half a decade). The party has not ended for Engineers wishing to join the AI cash grab; simply wrap OpenAI\u0026rsquo;s API with a few lines of JavaScript and sprint to your nearest investor with an open wallet. \u0026ldquo;Look my app summarizes {thing}!\u0026rdquo; \u0026ldquo;My chat app has a personality like John Wick!\u0026rdquo; and the checks pile up.\nBut remember I called this a moment of opportunity. Not for a short-lived buck, but an opportunity to carve out both soft landing and safe passage into the future. An opportunity not available to everyone; in fact, what makes the largely untenable nature for so many Engineers make it an opportunity for others.\nAI is a big space, but let\u0026rsquo;s narrow that view to predictive and generative models for the sake of conversation. If you look past the generative AI hype and ask what the larger set of AI tools offer, the answer is pretty simple - these tools offer a way to make computers do things that are really hard (or functionally impossible) to do without them. For a seasoned programmer this is not a new paradigm; every time you have adopted a new tool or learned a new language, it was because doing so expanded your plane of influence. Consider: at some early point in your career, you probably wanted to keep state between runs of your program but didn\u0026rsquo;t know how. So you learned how to interact with a database. Instantly, something that was impossible before (saving state) was now quite reasonable. It wasn\u0026rsquo;t free knowledge or magic - adding state to a program (along with the complexity of database adapters and idiosyncrasies) introduces new challenges, new elements for you to master in your design. But this ability of your program was unlocked. Today, you might want the results from your program to improve based on feedback from users. So you learn how to create an ML training cycle, applying user-provided scores to a dataset which is fed back to re-train your model. Instantly, something that was impossible before (progressive self-improvement of your program\u0026rsquo;s output) is now quite reasonable.\nDatabase and ORM evolution demonstrates a useful parallel; in the late 1980\u0026rsquo;s database innovation came from commercial powerhouses like Microsoft SQL Server and Oracle, and from OSS projects like Postgresql (and later MySQL). With the focus on OOP during the dot-com era, the first ORM and MVC patterns (both in Smalltalk) arrived in 1995. Even so, in 2004 many of us were still building PHP PDO models that amounted to 10,000 lines of stringified SQL queries. When you use any ORM today, you expect to be able to do things like\nuser.save() user.reset_password() intuitively, and not hand-craft queries like\ndatabase.run_query(\u0026#34;UPDATE user SET(u.first_name, u.last_name # etc for 30 more lines` The proliferation of MVC + ORM frameworks like Spring, Ruby on Rails and Django did not introduce any revolutionary database technology, nor did they represent a leap forward in computer science. They leveraged a very different technology, application design principals, and fostered a leap forward in the developer experience.\nI see the same challenges in the generative AI landscape today as post-dot-com web development: flush with technology, pale with structure. Instructor is a library that monkey patches OpenAI\u0026rsquo;s sdk to enforce response classes. Instructor released an ORM integration last week that allows database objects and LLM objects to be swapped directly. The project is tightly coupled to the OpenAI product, and I don\u0026rsquo;t see the ORM integration evolving past a novelty - a service generating db objects in a vaccuum is kind of pointless, and I think you\u0026rsquo;ll end up wrapping the classes and basically building them into oddly shaped tools calls to get any real value from it. But the idea does spark thoughts around what LLM software could look like, what form the abstractions should take on within a mature framework. This is the question Software Developers should all be asking right now, and it is a question the \u0026ldquo;old programmers\u0026rdquo; may be uniquely able to answer.\nWhat\u0026rsquo;s out there The crop of generative AI OSS projects fall largely into one of two categories: mostly academic or mostly inexperienced.\nThe mostly academic projects are driven by (no surprise here) data-focused researchers in academia. Projects like MemGPT are filled with technically brilliant concepts and truly revolutionary solutions to computing problems. What they lack is design patterns, implementation strategy, or any sense of a cohesive developer API. These are typically pseudocode-that-runs, a whiteboard thought exercise in how a solution might work squeezed into production. MemGPT is not a framework or a library, it is an exciting proof-of-concept.\nThe mostly inexperienced projects are motivated, fast-moving, and entirely focused on the wrong shiny objects. These are very high-profile, generally VC backed endeavors that tout the number of \u0026ldquo;tool integrations\u0026rdquo; - i.e. how many API wrappers they have shoehorned into the code base - as the benchmark by which they compete. This might make sense many cycles in the future, if the core project capabilities are sound bedrock (similar to the 5,000 integrations in the django-pakages registry). But with their current trajectory, I don\u0026rsquo;t foresee most of these projects ever reaching maturity. The contributors seem most impressed with what \u0026ldquo;coding can do,\u0026rdquo; focused on the novelty of giving an LLM access to Jira, less interested in ensuring that the framework produces robust, polished, reliable, and ultimately useful outcomes. The rush by countless gen AI projects to create a novel prompt templating solution is a fantastic example of this. If there is one universally solved problem in nearly every programming language, it is \u0026ldquo;how can I generate dynamic text within templated outputs?\u0026rdquo; Prompt rendering is significantly less complex than compiling valid SQL or HTML from partials, yet many of these libraries chose to roll their own or \u0026#x1f631; inject raw user inputs directly into the code. Understanding the difference between an unsolved problem and a problem you personally have not yet solved is a critical skill in Engineering, something that is painfully lacking in the open-source gen AI ecosystem today.\nBetween the two, I believe the opportunity moment for seasoned Developers lies within transforming the mostly academic projects first. These projects need veteran programmers to think through what a framework for this technology should look like, and execute the complex meta-programming that will make these tools accessible. Architecture decisions need to be guided by those that can consider the entire lifespan of the software and the whole of the developer experience, based on hard-won knowledge. The next true change agents in the gen AI space will be David Heinemeier Hanssons, not an Alan Turings. At the same time, evolution does not happen in a bubble and there will come a time (sooner than most think I believe) when the luster of the mostly inexperienced projects will collapse on their poorly built foundations; here too, the sage Engineers who are no strangers to righting the path of a lost ship will find their place.\nA perfect storm of opportunity But why \u0026ldquo;old programmers\u0026rdquo;? What makes this demand for framework-building, design-pattern-minded Software Engineers the exclusive domain of the graybeards? AI is supposedly a young person\u0026rsquo;s game, and only a few years ago we were informed that an impending army of bootcamp grads and CompSci majors were coming to commodize the field. How can it be argued that the next faces of change in AI look more like the cast of RED than Silicon Valley?\nFirst, meta-programming is hard. Really hard. It is the difference between knowing to use user.update(name='dave') when you want to save a name, and understanding the implications and side effects an update() function must address to work as expected. Most CS and boot camp grads are not meta-programmers; they enjoy a surface understanding of the tools they use, and are happy with a business-focused career progression; they will not go on to become \u0026ldquo;old programmers.\u0026rdquo; Instead, they graduate to Sysadmins and District Managers and CIOs, people that \u0026ldquo;used to code.\u0026rdquo; There is very little chance that these people will strike out to change the gen AI landscape any time soon.\nSecondly, I believe that there would be a glut of hyper-talented, reasonably experienced, lightly seasoned younger Software Engineers on the market today, had it not been for (F/M)AANG and their big tech ilk swooping in like the spacecraft from WAL-E to pacify and infantalize huge swaths of dev talent. Old programmers owe their good fortune, in part, to the opulent insanity of the last decade; every excess Engineer Facebook hired only to keep them off the market, rotting without work, is one less seasoned programmer on the market today. Junior devs that came of age in the 20/80 work/life \u0026ldquo;balance\u0026rdquo; practiced by far too many tech teams are still junior devs today, because that lack of seat time has translated into a lack of skills progression. That is not to say that no remarkably bright, highly skilled 26-year-olds escaped from Google with more knowledge and experience than I ever will posses - some absolutely did. But there are far fewer of them than should be, given the spike in popularity of CS degrees over the last decade, and the proliferation of boot camps, online programs, and free resources available to all.\nIf you are an old programmer - the kind of salt that still remembers what the letters in SOLID stand for, that kind that once had Patterns of Enterprise Application Architecture on your nightstand, the kind that actually understands DNS - then this is your call to service. Put down the world\u0026rsquo;s greatest Regional Engineering VP mug, dust off the keyboard, and jump into the fray of Artificial Intelligence Software Design where your skills are most needed. Help usher in the next generation of great framework design, and secure your place in a rapidly approaching future while you are at it. The opulent software world of past years needed software managers-of-managers, the AI-driven enterprises of tomorrow may not.\nthe old motorcycle salesman There is one more story to learn from in my motorcycle-selling history. An \u0026ldquo;old salesperson\u0026rdquo; who had been slinging bikes since the early 1980\u0026rsquo;s - let\u0026rsquo;s call him Rick. Rick benefited from the bubble as much as the next, purchasing a modest house and trading in his run-down Honda Civic for a new model. There came a point where it seemed like everyone was getting into the motorcycle selling game - realtors, car salespeople, even stock brokers were moonlighting with us. Rick was frustrated with this crowd - they knew nothing about the bikes they were selling, and they had no investment in the motorcycle community. After the crash, Rick left his sales manager role and went back to selling bikes. He had a little book that he would make calls from every day, and somehow a handful of customers would come in asking for him. When asked, he explained - it was an old Zig Ziggler sales strategy; every time he sold a motorcycle to a person he enjoyed working with, he would write down that person\u0026rsquo;s phone number and birthday in the book. Over a span of 20 years he filled the book until every day provided him a list of calls to make, people he already knew he liked - and almost every day\u0026rsquo;s list provided at least one customer who needed a bike, or knew someone that did. Rick the \u0026ldquo;old salesman\u0026rdquo; was there in the before-times. He was there for the crazy excess, and he reaped the benefits (within reason). He was there for the Indian summer, and smartly re-positioned himself while the rest squandered the opportunity. He comfortably settled into the new normal of motorcycle sales, which had no place for the rest of us (including myself).\nThe motorcycle industry is better with guys like Rick selling bikes, and Artificial Intelligence Software will be better with the guidance and grit of old programmers.\n","permalink":"http://0.0.0.0:1313/posts/an_indian_summer_for_old_programmers/","summary":"History The most money I have ever made was my years spent selling motorcycles. Software Engineering is by all means a lucrative profession, and I\u0026rsquo;ve never been the starving artist developer who prizes a compelling mission over a compelling paycheck. But the years I sold motorcycles - between 2003 and 2008 - coincided with a wave of temporary insanity in the US real estate market. Heretofore normal people became mad with the sudden access to \u0026ldquo;wealth,\u0026rdquo; in the form of massive home equity reserves they could spend as easily as cash via HELOC checkbooks.","title":"An Indian Summer for Old Programmers"},{"content":"It starts out innocent enough, on a frozen New Year\u0026rsquo;s Eve afternoon. You are doing some final documentation cleanup - because it is New Year\u0026rsquo;s Eve and not the best time for the team to be rolling out fresh feature code, and because updating the repo docs is usually a worthwhile endeavor in moments like this as part of good code hygiene. Being the clever and hilarious documentation author that you are, you hyperlink a picture of your dog Bash to a bash code block (pure pun magnificence, I might add). Happy with your updates, you wait for CI to pass testing, linting, scanning etc. and solicit a pull request rubber stamp from the team.\nYou are just about ready to close down your laptop when an alert pops up from AWS CodeDeploy: your deployment has failed. You assume it is a fluke - probably a failed resource allocation or something similar, all you did was add one line to a markdown file \u0026#x1f937;\u0026zwj;\u0026#x2642;\u0026#xfe0f;. So you retry. A few minutes pass, and then again the pipeline fails. Upon closer inspection the error indicates that your appspec.yaml file has a syntax error, and the deploy was not able to complete. You look in GitHub and the appspec.yaml has not changed for a month. An earlier pipeline with the same appspec.yaml built just fine three hours ago. Only one line of markdown has changed. What fresh hell is this? I\u0026rsquo;ll skip to the end (you can fill in several hours of searching and tweaking and pointless pull requests trying to find the suddenly offending character if you want the full experience): part of the CodeDeploy configuration is to specify artifacts to be used during the ECS blue/green deployment flow - and you can specify source and build artifacts. As you would expect, source artifacts are a copy of your repository code checked out in the source stage of your CodePipeline, while build artifacts are assets you created (and specified) during your CodeBuild stage. Since the appspec.yaml was not altered during the build, I had naively chosen to use the file directly from the repo those many months ago. However, the source artifacts have a hard size limit of 3MB; once your repository grows to larger than this, the artifact fails to import and silently returns nothing.\nLuckily, this \u0026ldquo;silent killer\u0026rdquo; is prominently documented in the CodePipeline docs \u0026hellip; just kidding, there\u0026rsquo;s no mention of it anywhere outside the troubleshooting guide. I appreciate the very Pythonic nature of that documentation style (don\u0026rsquo;t document the constraints, just explain them in the errors page). Of course there\u0026rsquo;s also no mention of the limit in the error - just that it cannot read the file. Happy New Year \u0026#x1f389;! The fix is relatively easy - include the file in the build artifacts, switch to build, call it a day. The frustration is that CodeDeploy (like so many AWS services) feels less and less like a managed service product and more and more like piggybacking on found code. As if someone built the source artifact to handle a very specific use case, and it worked, and so that was the end of the development cycle for that feature. There are only about 6 fields in the ECS Blue/Green deployment, and one of them can lead to your pipeline randomly failing when your codebase reaches a modest size. It is reflective of the AWS mentality of late - a complete fixation on shipping new services while barely supporting continuity for the old ones, with meaningful updates only for flagship products.\n","permalink":"http://0.0.0.0:1313/posts/the_one_where_a_picture_of_your_dog_broke_the_cd_pipeline/","summary":"It starts out innocent enough, on a frozen New Year\u0026rsquo;s Eve afternoon. You are doing some final documentation cleanup - because it is New Year\u0026rsquo;s Eve and not the best time for the team to be rolling out fresh feature code, and because updating the repo docs is usually a worthwhile endeavor in moments like this as part of good code hygiene. Being the clever and hilarious documentation author that you are, you hyperlink a picture of your dog Bash to a bash code block (pure pun magnificence, I might add).","title":"The one where a picture of your dog broke your CD pipeline"},{"content":"SQLAlchemy has nifty declarative mapping type support with the Mapped typing descriptor. If you are in the habit of typing your Python, this is a welcome tool that reduces boilerplate while still feeling quite natural:\nclass Banana(SqlalchemyBase): is_squishy:Mapped[Optional[bool]] = mapped_column(server_default=text(\u0026#34;TRUE\u0026#34;)) picked_date:Mapped[datetime] belongs_to: Mapped[\u0026#34;User\u0026#34;] = relationship(\u0026#34;User\u0026#34;, lazy=\u0026#34;selectin\u0026#34;, back_references=\u0026#34;bananas\u0026#34;) But interestingly, Mapped does not support forward refs outside the ORM. so picked_date:Mapped[\u0026quot;datetime\u0026quot;] will error with an ArgumentError if datetime was not otherwise imported in the module. However, our forward ref to \u0026quot;User\u0026quot; will be fine as long as you\u0026rsquo;ve imported User via TYPE_CHECKING.\nSo in conclusion, this works:\n... from typing import TYPE_CHECKING if TYPE_CHECKING: from . import User # avoids circular refs class Banana(SqlalchemyBase): is_squishy:Mapped[Optional[bool]] = mapped_column(server_default=text(\u0026#34;TRUE\u0026#34;)) picked_date:Mapped[datetime] belongs_to: Mapped[\u0026#34;User\u0026#34;] = relationship(\u0026#34;User\u0026#34;, lazy=\u0026#34;selectin\u0026#34;, back_references=\u0026#34;bananas\u0026#34;) but if you try to forward ref anything not inheriting from SqlalchemyBase, \u0026#x1f640;!\n","permalink":"http://0.0.0.0:1313/posts/fun_with_sqlalchemy_mapped_forward_refs/","summary":"SQLAlchemy has nifty declarative mapping type support with the Mapped typing descriptor. If you are in the habit of typing your Python, this is a welcome tool that reduces boilerplate while still feeling quite natural:\nclass Banana(SqlalchemyBase): is_squishy:Mapped[Optional[bool]] = mapped_column(server_default=text(\u0026#34;TRUE\u0026#34;)) picked_date:Mapped[datetime] belongs_to: Mapped[\u0026#34;User\u0026#34;] = relationship(\u0026#34;User\u0026#34;, lazy=\u0026#34;selectin\u0026#34;, back_references=\u0026#34;bananas\u0026#34;) But interestingly, Mapped does not support forward refs outside the ORM. so picked_date:Mapped[\u0026quot;datetime\u0026quot;] will error with an ArgumentError if datetime was not otherwise imported in the module.","title":"Fun with SQLAlchemy Mapped forward refs"},{"content":"Recently I went about standing up a fresh Continuous Delivery pipeline for a new project. The project is a relatively standard containerized stack with nothing exceptional to speak of, and as our exploratory work was already in AWS I decided to launch this CD in AWS CodePipeline. Furthermore, we wanted to dogfood our long-term infra management (this project is infra abstraction/automation software) and so opted not to use TerraForm or CloudFormation templates - that way we could have a fully \u0026ldquo;Clickops\u0026rsquo;d\u0026rdquo; infrastructure for our tools to reclaim and manage. Besides, Clickops is easy right?\nTo my surprise, nothing worked. Deploying CD for a very standard stack (details to follow) via GUI clicks is a bizarre dance filled with broken states and never-resolvable circular dependencies, magic file/variable names that are either not documented or documented incorrectly (like the variety of spellings for appspec.yaml / AppSpec.yml / appsec.yml across AWS CodeDeploy docs), cryptic error codes, and incompatible default settings. Some of this is nothing new - all the clouds have quirks. But I was taken off guard by just how bad, how completely non-functional I was suddenly finding these services which I had used happily for years.\nMaybe I have gotten too comfortable with infra as code, or maybe ECS and CodePipeline have gotten significantly more brittle in the last year. Maybe this is the multiverse where AWS services are much, much worse and I am a lost traveler. Whatever the reason, I decided that once this project was successfully deployed I would codify the process, in detail, as a guide for others (and my future self). This is that guide.\nHow To Guide: We are setting up a blue/green deployment of a containerized application. The application uses docker-compose locally with one application container, one db container, a sidecar container (in our case logging, but can be anything) and an nginx container to serve web content. In AWS our stack will be:\nCodePipeline to manage our blue/green deployment via CodeBuild and CodeDeploy A single ECS service running on Fargate A single Elastic Load Balencer ECR for storing our container images SecretsManager for managing our environment variables Github as our source code provider CloudFlare for our DNS and proxy provider There are numerous security groups, IAM roles etc needed in combination to make this all work, along with support elements like ACM to store the CloudFlare origin cert that I won\u0026rsquo;t directly call out here, but will be noted in the process.\nThis deploy will blue/green the whole of your container stack - this means if your sidecar(s) or nginx need to roll back they can also do so. The downside is that they are all built with every deploy. If you want to add more complex checking logic later to only build changed containers, go for it.\nDo not skim this guide! If you are like me, you normally skim over these things, copy the code examples, and refer back when you hit errors. Don\u0026rsquo;t do that. The ClickOps process here is like a ritual dance, missing one tiny step will anger the AWS gods and you will have to start over from the beginning. Suck it up and read the whole thing, and follow exactly step by step, or pain will follow.\nLegend To minimize repetition I am going to use some shorthands:\nThe \u0026#x1f9d0; icon means Follow directions exactly. I know, AWS docs or a tutorial might claim you can change the filename or specify the path somewhere; my experience has been that not all the configs are respected, and often only the default paths/names actually work. If you choose to deviate, best of luck to you. The \u0026#x2622;\u0026#xfe0f; icon means Thing for future us to improve on. This may mean tightening access scopes, moving elements to dedicated VPCs etc. This guide may be all that is needed for a small experimental startup, but you probably want to think about designated accounts / multi-region /mutli-cloud / multiverse in the future, and that\u0026rsquo;s beyond the scope here. The aws account number for the examples will be 123456789012. You can find-replace with your account. The project name will be bash-dog. You can find-replace with your project name. The AWS region will be us-east-2. You can find-replace with your default region. Steps Start your magic files. Our pipeline needs 3 files placed in the root directory of your project.\n\u0026#x1f9d0; The taskdef.json behaves similar to a docker-compose.yml in ECS \u0026#x1f9d0; The appspec.yaml file which pulls together the CodeDeploy deployment (Note: appspec.yaml is the correct, and AFAIK only working naming convention for this file). \u0026#x1f9d0; The buildspec.ymlfile which is similar to any CI actions file you may have used, it is basically a list of bash commands run in a \u0026ldquo;builder\u0026rdquo; context. also note that buildspec.yml and appspec.yaml have different filetype suffixes. Yay. Here are the files you should start with.\nI strongly suggest cloning them as-is and updating only the elements as you follow along. To quick copy them to your project root (where they must live):\ncd bash-dog/ # your project root curl https://gist.githubusercontent.com/norton120/61e9a94f035da8202ab74e41e1705087/raw/9b253c5ba06b05cbe1f6038d65b8690ffe088bd5/appspec.yaml \u0026gt; appspec.yaml curl https://gist.githubusercontent.com/norton120/61e9a94f035da8202ab74e41e1705087/raw/9b253c5ba06b05cbe1f6038d65b8690ffe088bd5/taskdef.json \u0026gt; taskdef.json curl https://gist.githubusercontent.com/norton120/61e9a94f035da8202ab74e41e1705087/raw/7ee3330bbbc33f3376cdc996a186cd0e90152e9a/buildspec.yml \u0026gt; buildspec.yml Commit these files to your codebase.\nMaking your project deploy-able This is a whole other topic in itself, so I\u0026rsquo;ll stick to the assumptions made with this current set of files and let you sort out what changes you may want to make:\nOne dockerfile, named Dockerfile, contains all the image definitions for your deployment. Each image to be deployed is targeted with the same name as the service you are deploying, i.e. the api image definition in your Dockerfile is defined as FROM some-image:tag as api. If this is foreign to you check out Naming your Builds. Your code builds environment-agnostic, that is, the same build runs locally as in production. Things like requirements files and entrypoints are managed via an environment variable and not a different set of build steps. Each service (i.e. container) runs on a different port. The awsvpc network addresses each of your containers via localhost (not the assigned container name!) so your containers cannot have port collisions. Only nginx will be exposed to the load balancer. Your nginx image needs to have an nginx.conf that routes traffic to all the other containers in your stack. Kinda goes without saying, but your nginx container should get port 80. We will be encrypting traffic from the load balancer to CloudFlare via origin cert (CloudFlare will handle client encryption), and restricting direct access to the container. Commit whatever changes you\u0026rsquo;ve made. For the duration of this deploy process it may save your sanity to turn off branch protection and deploy directly to main \u0026#x1f631;. Otherwise you will need to PR each tiny file tweak, and the PR process is basically valueless rubber-stamping in this case. ECR Images This whole build process centers around container images stored in Elastic Container Registry (ECR). Navigate to ECR in the AWS GUI and click \u0026ldquo;Get Started\u0026rdquo; under \u0026ldquo;Create a Repository.\u0026rdquo;\nName the repository with your project and service, i.e.bash-dog-api \u0026#x1f9d0; leave the defaults as they are. You need mutability to retag latest. Repeat for each of your services. You shouldn\u0026rsquo;t need to worry about docker hub rate limits, because our buildspec.yml preloads your existing image as part of the pre-build, so CodeBuild will only pull from docker hub the first time you build. If this does become an issue for some reason (you start getting failed builds because of dockerhub limits for images like niginx and python) then you will want to add ECR repos for these base images as well and point your Dockerfile towards them. That requires local login to ECR and complicates things, so we will avoid it for now. Just keep in mind that if you run into this issue, you\u0026rsquo;ll add these base images to everything we do here.\nTime to prime the pump: locally, log into your ECR registry with\ndocker login --username AWS --password \\ $(docker run \\ -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\ -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\ -e AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN} \\ -e AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \\ --rm amazon/aws-cli ecr get-login-password) \\ 123456789012.dkr.ecr.us-east-2.amazonaws.com/bash-dog-api note: this is using identity manager temporary creds. Use whatever strategy you prefer for authenticating the aws-cli container. Now build and tag your image:\ndocker build -f Dockerfile \\ -t 123456789012.dkr.ecr.us-east-2.amazonaws.com/bash-dog-api \\ --target api . note the --target needs to match the target we are building. Once the build is successful, push it up\ndocker push 123456789012.dkr.ecr.us-east-2.amazonaws.com/bash-dog-api:latest Rinse and repeat for each of the services you are deploying. Remember that bash-dog-nginx is our image of nginx with a custom nginx.conf mounted that will serve the sibling containers.\nSecrets We are going to keep all the envars in AWS SecretsManager. This removes secrets from our pipeline and repo code, and makes it less painful to update envars down the line.\nStart by navigating to SecretsManager and clicking \u0026ldquo;Store a new Secret\u0026rdquo; select other type of secret set the key/value pairs. This should be in the format of ENVAR_NAME value. For example, if I want an envar BASH_DOG_ENVIRONMENT with a value of production it would look like this: name the secret something logical like bash-dog/ecs-envars and create it. Once created, refresh the index page and click into the new secret so you can get the whole new arn (complete with the random suffix). Head back to your taskdef.json file. See the secrets section in the api and sidecar containers? Update as follows: for each envar you defined (and want for the given container), set a name key to match that envar (case sensitive). Update the arn with the whole new arn you just copied, paying special attention to the suffix on the arn. It must match the envar name you just set, and you need the 2 extra colons at the end. so setting our BASH_DOG_ENVIRONMENT envar in the api container would look like this: ... \u0026#34;name\u0026#34;: \u0026#34;bash-dog-api\u0026#34;, \u0026#34;secrets\u0026#34;: [ { \u0026#34;name\u0026#34;:\u0026#34;BASH_DOG_ENVIRONMENT\u0026#34;, \u0026#34;valueFrom\u0026#34;:\u0026#34;arn:aws:secretsmanager:us-east-2:123456789012:secret:bash-dog/ecs-envars-PlIIOb:BASH_DOG_URL::\u0026#34; }, ... remember that this arn will be the one you just created, not the example arn above!\nBe sure to delete any example envars you aren\u0026rsquo;t using, they will cause the pipeline to error if the secret or key does not exist. If you have envars that conflict between containers (for example, if each container needs a different HOSTNAME envar) you will need to set up individual secrets for each container. Then reference them with the same pattern, just using the container-specific secret arns for each set of secrets. Make sure references across containers use localhost and not the container names; this is one place I\u0026rsquo;ve found ECS awsvpc networking functions differently than a bridge docker network. so # bad API_HOST=http://api-container-name:8080 # good API_HOST=http://localhost:8080 Code Pipeline I find it is much easier not to get twisted into a dependency pretzel if we start our pipeline at the very end, with the CodePipeline itself.\nNavigate to CodePipeline -\u0026gt; CreatePipeline. Name your new pipeline something sensible like bash-dog-pipeline. Create a new service role, name it something sensible like bash-dog-pipeline-role. \u0026#x1f9d0; Leave \u0026ldquo;Allow AWS CodePipeline to create a service role so it can be used with this new pipeline\u0026rdquo; checked. (don\u0026rsquo;t try to re-use an existing service role, or hand-roll your own\u0026hellip; role. You will hate yourself if you do either of these things, and it just won\u0026rsquo;t work). Leave all the other defaults alone and click Next. In the next screen select Github (Version 2). Follow the prompts to create a new Github Connection, and select your repository. \u0026#x1f9d0; For Branch name use your main branch main or equivalent. Leave all the other defaults alone and click Next. Next, Create a build project inline by selecting AWS CodeBuild as the provider and then clicking Create Project. In the new window: name your build project something sensible like bash-dog-build-project. \u0026#x1f9d0; under Additional configuration check \u0026lsquo;restrict number of concurrent builds this project can start\u0026rsquo; and set the limit to 1. For Environment select Managed Image, EC2 Amazon Linux Operating system, Standard Runtime, and the \u0026#x1f9d0; amazonlinux2-x86_64-standard:4.0 image (not the default!) \u0026#x1f9d0; Check \u0026lsquo;Enable this flag\u0026hellip;\u0026rsquo; under Privileged. Leave the default New service role and unless the provided role name is awful, leave it. Optionally, reduce the timeouts. Generally my builds are running \u0026lt; 3 min, so if they are not done in 10 they are probably never going to be done. Leave Use a buildspec file and \u0026#x1f9d0; do not specify a file name. My experience has been that non-standard filenames for the buildspec/appspec/taskdef files have caused mysterious pipeline failures, but YMMV. Set up logging using logical names like bash-dog and codebuild. Skip the deploy stage for now, that needs to be backed-in from a running ECS Service. Save and create your new pipeline. Note: The pipeline will immediately build and fail. That\u0026rsquo;s OK, we\u0026rsquo;re far from done. Update Build Role Access Now we need to update the build service role, allowing it to: - login, pull and push our ECR images - write to our s3 artifact bucket (possibly already configured, but worth double-checking)\nFind that role you just created by searching IAM roles for bash-dog (well, your equivalent). It should look like codebuild-bash-dog-pipeline-service-role unless you changed it. It should also have a policy named something like CodeBuildBasePolicy-bash-dog-pipeline-us-east-2. Click into that. Edit the policy and add these statements, updating the image and checking that the s3 bucket arns for the artifacts are present and match what you have in s3. Run a successful build Push all the changes made so far to main in your application repo. If your main is already up to date, you will need to trigger it manually via the CodePipeline with the Release Change button. Let it build, check the logs tab for errors, and with fate on your side you should see this: Now for the fun part - navigate to the s3 bucket and find the path bash-dog-pipeline/buildArtf/. Look for an artifact with the newest timestamp. Download it. Now check out the taskdef.json file within the artifact. You\u0026rsquo;ll see the images have been updated to reflect the image sha for the release you just built! You can also check ECR and see that the same image tag was created.\nAdding your Cloudflare Origin Cert and Enabling Full Strict This seems random at the moment, but you will need this to create your load balancer, which you will do while creating an ECS service.\nLog in to Cloudflare for your respective domain. Under SSL -\u0026gt; Origin Certificate click Create Certificate. Leave the defaults. Note: the free certs will only work for bare domains (example.com) and single level subdomains (www.example.com). Click Create. In a different tab, navigate to AWS Certificate Manager and click on Import. Back in Cloudflare, time to copy the cert you just created, and paste the values into the respective boxes in ACR. \u0026#x1f9d0; click the copy buttons. Don\u0026rsquo;t try to scroll copy, you can grab/miss needed whitespace and break the cert. Leave Certificate chain blank. Click through to import the cert. Now you need to tell CloudFlare to use the cert you just created. This is important!. In CloudFlare, click into Rules -\u0026gt; Configuration Rules Create a new rule. \u0026#x1f9d0; Set the hostname filter to match exactly the (sub)domain fully qualified host (i.e. example.com or banana.example.com). \u0026#x1f9d0; Under then settings are\u0026hellip; find SSL, and select \u0026ldquo;Full (strict)\u0026rdquo; Save your rule OK now your CloudFlare is primed, though we have not set up the CNAME yet (that comes later). Onward with our pipeline. Create all the IAM Roles ECS Execution needs to be able to access the secret(s) created earlier, and ECS Service needs to be able to do normal ECS task things. CodeDeploy also needs a role. So, we create 3 new roles named bashDogServiceRoleForECS , bashDogExectuionRoleForECS , and bashDogCodeDeployRole in IAM. you may want/need to modify the AWS default ecsTaskExecutionRole instead of creating bashDogExecutionRoleForECS if you find your Fargate tasks do not reflect the permissions you\u0026rsquo;ve assigned to your execution role. The docs cryptically imply that Fargate does not respect your execution role - I have experienced it both working and not working with an alternate role, and truth be told haven\u0026rsquo;t had time to determine if this is a root cause.\nThe execution role (the role assumed by the host) needs: - AmazonECSTaskExecutionRolePolicy - An inline policy for accessing our secret envars Creating this policy is surprisingly unintuitive. Specify Elastic Container Service (no alias for ECS) and Elastic Container Service. The policy should look like this: Make sure your trust relationships looks like this:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ecs-tasks.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Next, create the Service role (think role assumed within the container). This is pretty straightforward, and is useful because you can add specific services to this role later as needed. This role needs AWSCodeDeployRoleForECS. Make sure the trust relationship of the created role looks like this:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ecs-tasks.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Last create the code deploy role. This role needs the AWSCodeDeployRoleForECS policy, and trust relationship should look like this:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;codedeploy.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Create an empty CodeDeploy Application We will need a CodeDeploy app for our ECS service to set up blue/green deploys in. So navigate to CodePipeline -\u0026gt; Applications -\u0026gt; and create a new application named something sensible like bash-dog-deploy-application. Leave this open. Don\u0026rsquo;t create a deployment yet, or you will get a weird loop where CloudFormation rips down your ECS Service for having too many deployments associated.\nConfigure Security Group and IAM Role Rules One of the most tricky elements of launching your ECS Service is that it really needs to succeed the first time you manually deploy it. If the initial task set for the service is unable to get to a \u0026ldquo;healthy\u0026rdquo; state, CloudFormation will stubbornly retry the same broken code until it finally tears down the service - and you have to start all over :sad_cat:. Before you attempt to launch the ECS cluster, go through all the dependent services and make sure you have set up security group ingress rules, access policies etc. ahead of time. Things like:\ndoes your app need database access, maybe RDS or Dynamo? have you set up ingress/egress policies for the the security group you will apply to the ECS cluster? does your app need other AWS services, like bedrock, or SQS, or Lambda? Have you applied all the needed policies to the bashDogServiceRoleForECS? are all the envars you need loaded into secrets manager? Try and get as far ahead of things that will break your application before you attempt to launch in ECS, you will be glad you did. Create the ECS Cluster and initial Task Definition Setting up the initial runtime is a little bit of a juggling act; You first create your ECS Cluster, Task Definition, and ECS Service with the Service linked to the Code Deploy (but not exactly managed by it yet). The idea is to manually stand up the service and get it to a \u0026ldquo;healthy\u0026rdquo; state, and then have CodeDeploy take over. 1. We create a new cluster from the ECS home page. Name the cluster something logical like bash-dog, leave all the defaults. This will take a minute, just leave it and wait until it is ready (clicking ahead will break things). 2. Next we create a task definition that will be used by our service. From the ECS home page click on Task definitions, Create a new task definition (with JSON). Paste the guts of your taskdef.json file (the one in your application repo) and save. 3. Finally we will create our ECS Service. Navigate to the bash-dog cluster and click Create under Services. - Leave the defaults for Environment. - Under Deployment Configuration select the task family name you just created. - Name the service something logical like bash-dog. - \u0026#x1f9d0; Under Deployment Options change Deployment Type to Blue/green deployment (powered by AWS CodeDeploy). - Leave the default deployment configuration of CodeDeployDefault.ECSAllAtOnce. - Search and select the arn for the Code Deploy role we created (bash-dog-code-deploy), you may have to go get the arn from IAM and paste it in the first time. - Under Networking select the default VPC, and de-select all the duplicate subnets (where there are more than one for a given zone). Use the primary subnets if you can. - Use your default security group (add this to the TODO list of things to harden later) - \u0026#x1f9d0; Leave public IP on. This isn\u0026rsquo;t just \u0026ldquo;expose ECS to a public IP and subnet so you can test it;\u0026rdquo; \u0026#x2622;\u0026#xfe0f; unfortunately, modern ECS Fargate cannot connect to any services (like ECR, SecretsManager etc) without either a public IP or NAT Gateway/PrivateLink setup in a private VPC. Long-term, having our ECS cluster hanging out publicly is not OK - but both solutions add complexity and cost we are not going to cover right now. I\u0026rsquo;ll try to do separate update on this topic later. - Under Load Balancer select Application Load Balancer - Name the load balancer something sensible like bash-dog - Bump up the grace period to make debugging easier - Select the nginx container bash-dog-nginx 80:80 - Create a listener on port 443 with https - Select the ACM cert we imported from CloudFlare earlier - Add primary and secondary target groups with the default http configs. Name them something sensible like bash-dog-main and bash-dog-secondary. - Save and wait for the new Service to spin up.\nService Cleanup: if when things go wrong with your service, you may have to either delete it and start over or it may decide to delete itself after several failed deployments. Be sure to completely remove all artifacts once the service is gone, by deleting the corresponding stack for the service in CloudFormation. Lingering artifacts (especially if you use the same sensible service name again when you retry) can result in all kinds of strange and sad behavior. This can take a while; be patient. Jumping ahead and re-building the new service while the old one is still deleting will cause you pain. At this point, if all goes well our service should be up and running. You can check in on the container logs via ECS Homepage -\u0026gt; Clusters -\u0026gt; bash-dog -\u0026gt; services -\u0026gt; bash-dog -\u0026gt; logs. We can now wire up the domain and make sure the initial deploy is working!\nSet up CNAME Record Head over to the load balancer we created - you can find it by navigating to the ECS Service homepage and clicking on the listener, then from there the load balancer. Grab the DNS name of your load balancer. You can throw this in a browser and get an unsafe warning (which is fine, the cert it is using is made for CloudFlare not for visitors). If you bypass that warning, you should see your application!.\n14 .#### Completing the pipeline Time to automate your now working-but-manual pipeline.\nHead back to CodePipeline, and in your bash-dog-pipeline and edit the pipeline. add a deploy stage. \u0026#x1f9d0; Select Amazon ECS (Blue/Green) as provider. For artifacts select BuildArtifact For AWS CodeDeploy application name and AWS CodeDeploy deployment group you want to select the app and group created by your ECS Service; these should have a random prefix, like AsFbaw-bash-dog-bash-dog. For the Amazon ECS task definition put \u0026#x1f9d0; exactly taskdef.json. For the AWS CodeDeploy AppSpec file put \u0026#x1f9d0; exactly appspec.yaml Leave Dynamically update task definition image - optional alone, do not add or change anything. Save your new stage (3 saves to get all the way out), confirming the changes to the pipeline. Under CodeDeploy -\u0026gt; Applications -\u0026gt; name_of_generated_application -\u0026gt; Deployments, click into the active deployment. scroll down to Deployment Settings at the bottom and set the original revision termination to something rational, like 5 minutes. Otherwise every deploy will take over an hour to be \u0026ldquo;complete.\u0026rdquo; Save and exit. OK. Now the big moment. Push some code to a branch, create a pull request, merge that into main and\u0026hellip;. You have a working, automated blue/green deployment for all 3 of your service containers.\n","permalink":"http://0.0.0.0:1313/posts/complete_cd_with_aws_codepipeline_ecs_and_cloudflare/","summary":"Recently I went about standing up a fresh Continuous Delivery pipeline for a new project. The project is a relatively standard containerized stack with nothing exceptional to speak of, and as our exploratory work was already in AWS I decided to launch this CD in AWS CodePipeline. Furthermore, we wanted to dogfood our long-term infra management (this project is infra abstraction/automation software) and so opted not to use TerraForm or CloudFormation templates - that way we could have a fully \u0026ldquo;Clickops\u0026rsquo;d\u0026rdquo; infrastructure for our tools to reclaim and manage.","title":"A Complete Containerized CD Pipeline With AWS and Cloudflare for 2024"},{"content":"It is difficult to argue substantially that there is not enough time if there is a TV on your wall or TikTok on your phone.\n","permalink":"http://0.0.0.0:1313/posts/on_having_enough_time/","summary":"It is difficult to argue substantially that there is not enough time if there is a TV on your wall or TikTok on your phone.","title":"On Having Enough Time"},{"content":"I have a working theory that most of the Data Scientists working today developed their naming conventions by watching Star Trek: The Next Generation in the 90s.\n","permalink":"http://0.0.0.0:1313/posts/why_data_scientists_are_so_bad_at_naming_things/","summary":"I have a working theory that most of the Data Scientists working today developed their naming conventions by watching Star Trek: The Next Generation in the 90s.","title":"Data Science Nomenclature"},{"content":"Consulting and Contract Work There is no substitute for hard-won experience, no shortcut for seat time. If you need Engineers with a track record of success in Artificial Intelligence, Machine Learning and Data Engineering, you need Pirate Baby.\nWhat We Can Do Together \u0026#x1f916; AI/ML Ops: The \u0026ldquo;AI Gold Rush\u0026rdquo; has created a lot of prospectors - don\u0026rsquo;t let your project be some Engineer\u0026rsquo;s practice run. Real ML pipelines, effective model development and delivering legitimate, testable, observable AI results come with a steep learning curve; at Pirate Baby we talk about the ML products we have successfully shipped and the lessons we have already learned.\n\u0026#x1f4c8; Data Analytics: From data warehouses to data mesh, EL pipelines to DBT, Pirate Baby has a concrete foundation in the traditional Data space. As AI chips away at transformation tedium and closes the Natural-Language-to-SQL gap, it has never been more important to keep one foot in the present and one in the future.\n\u0026#x1f3d7;\u0026#xfe0f; Infrastructure: It is bigger than just AWS/GCP/Azure now. Megaclouds are still your core services, but 2024 will see hyper-specialized cloud providers strike back. We are here to implement or augment your IAC solution, regardless of service provider.\n\u0026#x1f4bb; Developer Experience: Development environments should be Software Engineering force multipliers, catered to the needs of your team. At Pirate Baby we have over a decade of experience building, refining, and containerizing development processes to eliminate tribal knowledge, measuably shrink cycle times, and make onboarding delays a thing of the past. We will fit the tools to empower your people, not the other way around.\nCommitment to the Bleeding Edge The Generative AI space is evolving at a breakneck pace. Experts must \u0026ldquo;live at the tip of the spear\u0026rdquo; by actively contributing to ongoing Open Source Software and academic projects, or risk falling behind. At Pirate Baby we are seriously committed to laying the path forward for Generative AI Software; this means devoting as much as 50% of our cycles to the OSS and academic projects that are defining the GenAI landscape.\nMaximizing ROI with Pirate Baby Whenever working with external resources it is critical that you maximize your return. Here are some important ways you can do that.\nminimize meetings: Invest in async communication tools like Loom, and provide our team with need-to-know summaries instead of paying a premium for us to sit in on irrelevant calls. clearly define work: Have work well understood and well documented before assigning it to our team. A clearly stated Definition of Done allows us to work efficiently and \u0026ldquo;stay in flow\u0026rdquo; to deliver you the best possible product in the shortest time. articulate problems, not solutions: We are really good at managing our time. If we understand what problems we need to help you solve, we can plan how to solve them as efficiently as possible. Get In Touch Think Pirate Baby is a good fit for you and your team?\n\u0026#x2709;\u0026#xfe0f; drop us a line\n","permalink":"http://0.0.0.0:1313/work-with-pirate-baby/","summary":"hiring Pirate Baby for consulting or contract engagements","title":"C\u0026C"},{"content":"There is a well-warn platitude that \u0026ldquo;debt is stealing from your future self.\u0026rdquo; The logic follows that instead of (or worse, in addition to) spending the resources you currently have, debt is the act of spending against resources you will have down the road. Debt functions on highly optimistic principals, namely that future you will be so very well off that not only will that version of yourself have enough means to cover your then-current expenses, but also surplus to cover the now-current expenses present-day you is unwilling or unable to afford.\nIn terms of productivity, mental capacity and energy, caffeine is debt in liquid form. We tend to think of coffee and espresso shots as a fuel or energy source - something additive that picks us up, makes us faster/sharper/more alert. In truth caffeine is reductive, functioning not by adding energy but by silencing the brain\u0026rsquo;s exhaustion signals, muting the alarms that our brains and bodies need rest in a form of biological debt, one that will need to be repaid.* The compound adenosine serves as the biological sand in our metaphorical hourglass, collecting in our nervous systems during waking hours until we sleep and the chemical clock is reset. Adenosine is our chemical gas gauge, letting us know when we are mentally running on fumes. Enter caffeine; chemically able to bind to adenosine receptors, caffeine acts to inhibit our ability to accurately gauge fatigue. This is tantamount to covering your car\u0026rsquo;s temperature warning light with magic marker when your engine is overheating. This overloading act has similar effects as any form of debt. By taking possession of energy and alertness that does not belong to present-day you (but is the rightful property of a future version of yourself), you make the assumption that future you will have ample energy and rest to spare. Sadly this is rarely the case; sometimes we can steal back rest on weekends or crash for a day after a hard-burn, but more often than not, the future version of us is also amassing debt in the form of late nights and lattes. The cyclical levying of energy tax on our future selves accrues interest, as we are not actually performing at peak during those stolen hours. We are tired, even if we have fooled our bodies\u0026rsquo; analytics into thinking otherwise. Our ability to reason and execute is compromised at a biological level - that is the reason our adenosine fuel gauge was installed in the first place. So each day we wake, burdened with the debt assigned by past-us and the sub-par work produced by a compromised mind and body, and we double down with more shots in our venti red eye.\nDebt can be a strategic tool in moments when it is legitimately viable that future you will have significantly more resources than current you. A business with massive (recoverable) accounts receivables but little cash-on-hand can leverage debt to bridge the gap. But as a sea of failed zero-rate startups learned the hard way, accruing debt against \u0026ldquo;likely future success\u0026rdquo; is a dangerous game.\n*yes, technically caffeine acts as a mild stimulant in addition to the aforementioned fatigue-blocking properties. While not totally inconsequential, there are plenty of other mild stimulants that draw comparatively little interest: we don\u0026rsquo;t see the world consuming 1.2 tons of ginseng or yerba mate a year, and there\u0026rsquo;s a reason for that.\n","permalink":"http://0.0.0.0:1313/posts/caffeine/","summary":"There is a well-warn platitude that \u0026ldquo;debt is stealing from your future self.\u0026rdquo; The logic follows that instead of (or worse, in addition to) spending the resources you currently have, debt is the act of spending against resources you will have down the road. Debt functions on highly optimistic principals, namely that future you will be so very well off that not only will that version of yourself have enough means to cover your then-current expenses, but also surplus to cover the now-current expenses present-day you is unwilling or unable to afford.","title":"Caffeine"},{"content":"So you want to comment on something we\u0026rsquo;ve posted? Sick. We\u0026rsquo;re glad you are here and eager to hear your thoughts. But before you jump in, let\u0026rsquo;s go over some ground rules to keep this place chill for everyone:\nThe Golden Rule of Commenting Keep it Clean: Let\u0026rsquo;s keep things PG-13 rated. No need for anything obscene or inappropriate. Profanity is a spice, not a main ingredient. Stay on Topic: While we love a good tangent, let\u0026rsquo;s try to keep our comments related to the blog post. If you\u0026rsquo;ve got something totally different to chat about the world is full of places to share that. No Advertisements or Solicitation: Spam sucks. So, please, no sneaky advertisements or promotions. This space is for genuine conversations, not sales pitches. If you do it you\u0026rsquo;ll be assumed a bot (see rule 4). No Bots: Comments are for humans, full stop. If you bot, you will be banned. Respect is Key: The most important point of view to hear is one you don\u0026rsquo;t agree with. Be respecfully, disagree respecfully, or go away. No personal attacks, flame wars or any of that nonsense. Keep it Legal: Steer clear of anything shady or illegal. That means no sharing of pirated content, no threats, and definitely no illegal activities. No Trolling Zone: We\u0026rsquo;re all for a good joke, but trolling? Nah, not here. Let\u0026rsquo;s keep the trolling to a minimum and focus on meaningful discussions instead. So, there you have it - follow these simple guidelines, and we\u0026rsquo;ll all have a great time hanging out in the comments section. Can\u0026rsquo;t wait to hear what you have to say!\n","permalink":"http://0.0.0.0:1313/comment-policy/","summary":"Rules for commenting on blog posts","title":"Comment Policy"},{"content":"Let me preface this by saying that I very much appreciate the entire Lang* ecosystem. At my shop we rely heavily on Langfuse for development as well as the tagging interface for our training strategy. More than a few platforms have tried to be the framework for language model applications (think Guidance, Haystack etc), and the Lang* team has arguably made the most headway, and done it OSS \u0026#x1f918;. So try not to disregard these observations as contrarian or success-hating; I really want to love LangChain.\nWhen we began work on our inference engine back in October 2023 (so a millennia ago in LLM time) LangChain was the obvious choice. By the first week of December we had abandoned the framework, and by Christmas we had written our own internal prompt templating module to support a Q1 product launch. Such a drastic architectural change at an equally critical time in the product life cycle is not a thing to take lightly. But the growing collection of red flags became something we could not ignore, and I believe we made the best decision.\nWhat red flags?\nDependency Management If you\u0026rsquo;ve been around the Python ecosystem for more than a minute, you know that dependencies are the Achilles\u0026rsquo; heel of the language. This is true for Python as a whole, but I have found this to be exponentially worse in the data sphere. When Software Engineering, Data Engineering and Data Science coalesce, you find deeply abstracted package dependencies with pins like Cython=\u0026gt;0.1 that \u0026ldquo;worked before,\u0026rdquo; and lurk in the darkness of deployed code waiting to ruin your weekend. A few years ago the snowflake-sqlalchemy adapter was so often the source of surprise dependency failures that my team would comment we're not snowflake whenever a bad pin made it to a PR review. The general wisdom is to minimize the surface area of dependencies by importing only what your code needs to survive. LangChain does the opposite of this. The framework currently boasts over 200 supported integrations, every one of them a dependency. If something breaks in the requirements management of the Discord integration, it breaks your project, regardless of if you use Discord or not. In November of 2023 this resulted in what is essentially a text compiling framework that takes 18 minutes to build in a container. It was a hopeful sign when LangChain announced that the framework would be split into langchain-core and langchain-community packages - great! dependency isolation! But as of now that split is still largely academic. Yes there is a langchain-core package, which in itself provides little functionality and still feels extremely coupled to the community ecosystem (more on that in a moment). But if you want any of the integration support you are forced to once again adopt the whole wad of integration spaghetti - back to square one. Before LangChain announced a December 7th date to release langchain-core, I had taken two days in November to fork the library and try to carve a core implementation out myself. That\u0026rsquo;s when I discovered the next of our red flags.\nOnion Wrapper Architecture There are dozens of \u0026ldquo;I re-implemented LangChain in 40 lines of code\u0026rdquo; posts on Reddit and Medium; I would love to dismiss them all as clickbait, but in truth our prompt templating engine is a couple hundred lines of readable code, and most of our composable \u0026ldquo;links\u0026rdquo; that make up our chains are less than 30 lines. The LangChain codebase is understandably rushed (2023 was the AI land grab, and a lot of hard choices are made when speed is so critical). But there is a specific pattern, one quite common with inexperienced Software Engineers under pressure, that doesn\u0026rsquo;t just pepper the code base - it is the primary flavor. That flavor is onion wrapper architecture. It goes something like this:\nI get a thing working for an extremely specific use case I need some of the functionality from that thing, or all of it in a different context, but it has been a minute and I don\u0026rsquo;t really understand how the first thing works. So I import the first thing, wrap it in another thing in my new module, then expose that thing I need parts from the second thing, but don\u0026rsquo;t really understand how that thing works because it\u0026rsquo;s a sort of mangled version of the first thing, so I import that second thing into a third module and wrap that with more code that works, at the moment. Rinse and repeat, until changing a single quote in a prompt requires you to have 35 files with 1100 lines each open in your text editor. There are a ton of reasons this kind of design happens - unreasonable delivery demands being one of them, a wide-open-door contribution policy being another. We have all worked on application codebases that look like this - they are the ones that crash all the time with no warning, take forever to get back up by piling more spaghetti on the mound, and are nearly impossible to add new features or fixes to. Behaviors are hidden deep in the stack (\u0026ldquo;where did this random timeout setting come from?!?\u0026rdquo;), and side-effects take on a black magic all their own. The idea of building on a framework that looks like this\u0026hellip; that is frightening. Frameworks are your bedrock, they are the solid base on which applications are erected. Framework code is arguably the highest form of software development, meta-programming at its most impactful. Frameworks are not a place to ship-now-at-all-costs, they are the code that must be reliable for the applications that depend on them to flourish.\nNot an Application Framework LangChain describes itself as a \u0026ldquo;framework for developing applications powered by language models.\u0026rdquo; I disagree. LangChain is a software package, a collection of modules, but it lacks the core elements of what can be reasonably called a framework. It is one thing to develop reusable code elements (these are modules), another thing to make that code portable (these are packages). A framework goes beyond \u0026ldquo;here\u0026rsquo;s a way to access some functions you can use.\u0026rdquo; It is an inherent architecture, a structure that manages how parts of your code interact with each other. Airflow is a framework - it uses file system organization to create a familiar pattern across airflow instances, makes opinionated design decisions that ensure the application will function as a whole if adhered to. FastAPI is a framework that does not leverage the filesystem - it has a clear pattern using routers, routes, lifecycle events and dependencies. Even with a non-standard implementation of FastAPI, most developers can grok what is happening and where with few ctrl+f searches for baseline elements. Frameworks answer the very difficult question of \u0026ldquo;how should the parts of my code work together?\u0026rdquo;. LangChain does none of this, and is functionally closer to a library like requests than it is to an application framework like Next.js or Rails. The most glaring example of the is the LangChain documentation, where every example exists as a notebook cell. This reflects how LangChain is designed - specifically for isolated scripting, and not as a component in a multifaceted application. Pick any framework - Django, Phoenix, Electron - and note how many points in the documentation refer to multiple files:\n# my_route.py from my_model import Bla ... all_the_foos = await Bla.awaitable_attrs.foos # my_model.py class Bla(Base): foos:list[Foo] = relationship(...) When your framework is designed as a pile of scripts, your application becomes a huge pile of scripts.\nHuh? Moments This was the last red flag, the proverbial nail in our LangChain-shaped coffin. About the time we were realizing that the larger langchain package was not what we wanted and began attempting to extract langchain-core on our own, a similar discussion thread appeared within the LangChain repo. We were excited to see the parity of thought! But a comment regarding version strategy stood out to all of the team: I read this as \u0026ldquo;_we are going to break things all the time and we\u0026rsquo;d rather just ship whatever whenever and not worry about the impact of regressions on your code.\u0026rdquo; The space is changing fast, and because of that fact correct versioning is critical. Many of the language-model-related packages we use are already several majors behind from only a few weeks ago - and that is our problem, because the space is moving fast and we need to decide when we will do the painful upgrades. But deciding, as a library, not to correctly version because braking changes are expected often\u0026hellip; that\u0026rsquo;s like saying \u0026ldquo;we know the probability of a heart attack is really high, so we\u0026rsquo;re going to get rid of your fitbits so we don\u0026rsquo;t need to deal with warning signs.\u0026rdquo;\nI have hope A month after we made the decision to abandon LangChain, much has changed. V.0.1.0 was released, promising a stable entrypoint and, eh, interesting (non-semantic) versioning (but hey at least they set rules?). LangChain is moving to a sensible stand-alone packages model for integrations. It would be amazing if we see the project mature into either a stable, reliable library, or evolve into a true llm-application framework. And I\u0026rsquo;ll be the first person to condemn our bespoke code to reside in git history and wire up a Lang*-based chain, when (and if) that day comes. But I am confident that the LangChain of right-at-this-moment will be the very center of unreliable, painfully brittle code bases that organizations are desperate to get away from this time next year.\n","permalink":"http://0.0.0.0:1313/posts/langchain_is_ticking_timebomb/","summary":"Let me preface this by saying that I very much appreciate the entire Lang* ecosystem. At my shop we rely heavily on Langfuse for development as well as the tagging interface for our training strategy. More than a few platforms have tried to be the framework for language model applications (think Guidance, Haystack etc), and the Lang* team has arguably made the most headway, and done it OSS \u0026#x1f918;. So try not to disregard these observations as contrarian or success-hating; I really want to love LangChain.","title":"Langchain is a Ticking Timebomb"},{"content":"The world never ceases to be filled with new things, and that is exciting. New things are great, but as they come with a learning curve - not the least of which is the language surrounding them. Like when the first coffee shop opened in the little town where I went to college in the late 90s, and we learned to order â€œexpress-oâ€ (which was named that, logically, because it was like coffee but faster).\nSuch is the case today with words like generative AI, large language models, and of course prompt engineering. These words get tossed into every plan and pitch deck with grand assumptions and promises of near-magic; the ratio of experience to conjecture is painfully low, and the noise from an army of pseudo-knowledgeable content creators looking to cash in on the AI gold isn\u0026rsquo;t helping.\nTake the case of ChatGPT, the most widely recognised LLM on the market. Except\u0026hellip; ChatGPT isn\u0026rsquo;t an LLM. ChatGPT (and the accompanying OpenAI API) is a brilliantly engineered software platform, representing billions of dollars of R\u0026amp;D, and hundreds of thousands (if not millions) of software development hours. ChatGPT is as much an LLM as Google Search is a NoSQL database. Yes, one of the many complicated parts of the ChatGPT platform is a set of proprietary Large Language Models, just as Google Search has databases of a sort. When you query the OpenAI API, you are interacting with a production consumer-facing endpoint, not a raw LLM. Understanding this - the massive delta between an OSS LLama2 or Mistral or Falcon model running on AWS Bedrock or GCP Vertex, and querying a polished, consumer-facing product, is tantamount to separating the generative AI wheat from the chaff.\nNow on to Prompt Engineering. Not \u0026ldquo;how to phrase your question to get the best answer from ChatGPT,\u0026rdquo; which I wish everyone would start calling what it is, product training, but designing model-level textual interaction templates. Here\u0026rsquo;s a real-world example from earlier this week: the key players are myself (a Data Engineer living in a Data Science world), Woodson (our actual Data Scientist), and a stock Falcon-7b-instruct instance. I started early in the morning, trying to fix a stubborn link in our prompt chain:\nprompt:\nYou are a helpful AI assistant.Assess if the question below is a complete thought that can be responded to without any external context. Respond only \u0026#39;YES\u0026#39; or \u0026#39;NO\u0026#39;. \u0026gt;\u0026gt;QUESTION\u0026lt;\u0026lt; That is fine, just pick one. \u0026gt;\u0026gt;ANSWER\u0026lt;\u0026lt; response:\n%YES,no\\nI agree it is fine but th K. No good. Try again with all the same settings for temperature, stop words etc. prompt:\n\u0026gt;\u0026gt;INTRODUCTION\u0026lt;\u0026lt; You are a helpful AI assistant.if the question below is a complete thought that can be responded to without any external context respond YES, otherwise NO. \u0026gt;\u0026gt;QUESTION\u0026lt;\u0026lt; That is fine, just pick one. \u0026gt;\u0026gt;ANSWER\u0026lt;\u0026lt; response:\n**lk)! Umm.. that\u0026rsquo;s.. worse? Maybe better, because it is shorter? OK again, maybe no special tokens this time, and raise the temperature. prompt:\nIf the message below is a complete thought that can be responded to without any external context respond YES, otherwise NO. MESSAGE: That is fine, just pick one. response:\nYESyes\u0026lt;backpage Closer, but also incorrect, and still not stopping where I want it. This goes on for about an hour, adjusting the prompt and the settings. I finally get to this:\nDetermine if this message is a complete statement that requires no external context, respond only with \u0026#39;yes\u0026#39;, otherwise respond only with \u0026#39;no\u0026#39;. MESSAGE: That is fine, just pick one. response:\nNo WOO! Now I run it 5 more times in a row. No, Yes, YES, YesNo, Yes\nAt this point I start thinking about re-entering a career in the food service industry.\nJust then Woodson walks into the office. He looks over my head (which is rested firmly forehead-planted on the desk while my fist bangs the keyboard) and scans the prompt terminal of my massive computer screen.\n\u0026ldquo;Where\u0026rsquo;s the temp set to? OK drop that to point one. Invert the question to ask if the message requires external context, add a second sentence with yes or no, lowercase, get rid of that period, and \u0026hellip;(pauses to slurp iced tea)\u0026hellip; make the word message lowercase, get rid of the space in front of the colon. Run it.\u0026rdquo;\nprompt\nDoes this message require external context for you to respond? Answer yes or no message:That is fine, just pick one. responses (x5) Yes, Yes, Yes, Yes, Yes (this is correct, remember we inverted it)\n\u0026ldquo;How did you do that?!?\u0026rdquo;\n\u0026ldquo;Wish I could tell you man. I just did.\u0026rdquo;\nHe went on to talk about a need to consider narrowing the conversational focus, sounding more like Maharishi Mahesh Yogi explaining the mechanics of transcendental meditation than a Software Engineer talking compiler bugs. I retained as much of what was said as you read above.\nThis is a new sport. This kind of practice requires a different kind of thinking, and I am not entirely convinced it can be effectively translated through teaching. Maybe it will be like music; most of us can take tuba lessons and be part of the middle school band, but few will ever be good enough to play music professionally at any level - not just the proverbial (and in this case, actual) rock stars, but even so much as to cover the bills of a modest lifestyle. To be able to craft text that effectively coaxes gold from language model mire is, at the moment, a bit of an art form. There are likely people all over the globe that have never considered an interest in programming or computer science, but are naturally exceptionally gifted at this new dark art. There are people working a New Jersey gas pump, or pulling lobster pots from a bay in Maine, or entering medical claims in an office in Tulsa, that would look at the same block of text and say \u0026ldquo;yeah just invert the question, and that word needs to be lower case\u0026rdquo; like it was placing the last piece in a jigsaw puzzle. Wilt Chamberlain was a bellhop when the world of professional basketball first discovered him.\nI guess it is all relative; to some developers, SQL is database incantation, or JavaScript is a world of broken promises (pun intended). I personally believe that wood framing, electrical work, and plumbing are logical skills that can be learned, shared, and practiced, whereas any task involving drywall or spackle requires a pact with the devil to produce a half-way decent looking outcome.\n","permalink":"http://0.0.0.0:1313/posts/prompt_engineering/","summary":"The world never ceases to be filled with new things, and that is exciting. New things are great, but as they come with a learning curve - not the least of which is the language surrounding them. Like when the first coffee shop opened in the little town where I went to college in the late 90s, and we learned to order â€œexpress-oâ€ (which was named that, logically, because it was like coffee but faster).","title":"Prompt Engineering"}]